{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "cZzXC7IHCazM",
        "ET8xcMK0WEeJ",
        "KLhzjCifTcW5"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFpgp_rIA_TY"
      },
      "source": [
        "# Land Use and Land Cover Classification using Pytorch\n",
        "**Content Creators**: Isabelle Tingzon and Ankur Mahesh\n",
        "\n",
        "Welcome to CCAI's tutorial on land use and land cover (LULC) classification using Pytorch!\n",
        "\n",
        "In this two-part tutorial, you will learn how to:\n",
        "- train a deep learning model for image classification using Pytorch\n",
        "- generate land use and land cover maps using Python GIS\n",
        "\n",
        "You can make a copy of this tutorial by File→Save a copy in Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "\n",
        "\n",
        "*   [Overview](#overview)\n",
        "*   [Climate Impact](#climate-impact)\n",
        "*   [Target Audience](#target-audience)\n",
        "*   [Background & Prerequisites](#background-and-prereqs)\n",
        "*   [Software Requirements](#software-requirements)\n",
        "*   [Data Description](#data-description)\n",
        "*   [Methodology](#methodology)\n",
        "*   [Results](#results)\n",
        "*   [Exercises](#exercises)\n",
        "*   [References](#references)"
      ],
      "metadata": {
        "id": "gchK901cq3hN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nuv5XES8Ejc_"
      },
      "source": [
        "<a name=\"overview\"></a>\n",
        "# Overview\n",
        "This tutorial covers an introduction to image classification using Pytorch for land use and land cover (LULC) mapping.\n",
        "\n",
        "Specifically, you will learn how to:\n",
        "- classify satellite images into 10 LULC categories using the [EuroSAT dataset](https://arxiv.org/abs/1709.00029)\n",
        "- fine-tune a Resnet-50 CNN model for image classification\n",
        "- save and load trained models in Pytorch\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"climate-impact\"></a>\n",
        "# Climate Impact\n",
        "A [report](https://www.wri.org/insights/7-things-know-about-ipccs-special-report-climate-change-and-land) by the World Resources Institute (WRI) states that about 23% of global human-caused GHG emissions come from land uses such as agriculture, forestry, and urban expansion. Land use change such as deforestation and land degradation are among the primary drivers of these emissions. Rapid urbanization leading to an increase in built-up areas as well as a massive loss of terrestrial carbon storage can also result in large carbon emissions.\n",
        "\n",
        "Mapping the extent of land use and land cover categories over time is essential for better environmental monitoring, urban planning and nature protection. For example, monitoring changes in forest cover and identifying drivers of forest loss can be useful for forest conservation and restoration efforts. Assessing the vulnerability of certain land cover types, such as settlements and agricultural land, to certain risks can also be useful for for disaster risk reduction planning as well as long-term climate adaptation efforts.\n",
        "\n",
        "With the increasing availability of earth observation data coupled with recent advanced in computer vision, AI & EO has paved the way for the potential to map land use and land cover at an unprecedented scale. In this tutorial, we will explore the use of Sentinel-2 satellite images and deep learning models in Pytorch to automate LULC mapping.\n",
        "\n",
        "<br>\n",
        "<center><p><p> <img src=\"https://ptes.org/wp-content/uploads/2018/04/iStock-664630460-e1524839082464.jpg\" alt=\"alt\" width=\"50%\"/>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6yqHcIOjZD2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"target-audience\"></a>\n",
        "# Target Audience\n",
        "This tutorial is intended for experienced and aspiring data scientists looking for concrete examples of the applications of deep learning in climate change. We expect users to have some background in machine learning, including neural networks. But don't worry if you are new to these topics! We will provide additional resources and external links below for you to further study."
      ],
      "metadata": {
        "id": "xyb28HbsrcLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"background-and-prereqs\"></a>\n",
        "## Background and Prerequisites\n",
        "For a refresher on neural networks, feel free to check out the video below by [3Blue1Brown](https://www.youtube.com/c/3blue1brown).\n",
        "\n",
        "We also highly recommend [Stanford's lecture collection](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) on convolutional neural networks (CNNs) for visual recognition. The deep learning specialization courses at [deeplearning.ai](https://www.deeplearning.ai/courses/) also provide an in-depth introduction to ANNs, CNNs, sequence models, and other deep learning concepts."
      ],
      "metadata": {
        "id": "AaV01KXRrGAe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "HxshqdcpvvHB",
        "outputId": "28e31069-1f92-468f-b89a-4f5627b1128f"
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('P28LKWTzrI')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x79e172e9db10>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"400\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/P28LKWTzrI\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"software-requirements\"></a>\n",
        "# Software Requirements\n",
        "\n",
        "This notebook requires Python >= 3.7. The following libraries are required:\n",
        "*   tqdm\n",
        "*   pandas\n",
        "*   numpy\n",
        "*   matplotlib\n",
        "*   pytorch"
      ],
      "metadata": {
        "id": "PMhFV8gzrqO-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H08M_-5iEbc1"
      },
      "source": [
        "## Enabling GPU in Google Colab\n",
        "Before we start, you will need access to a GPU.  Fortunately, Google Colab provides free access to computing resources including GPUs. The GPUs currently available in Colab include Nvidia K80s, T4s, P4s and P100s. Unfortunately, there is no way to choose what type of GPU you can connect to in Colab. [See here for information](https://research.google.com/colaboratory/faq.html#gpu-availability).\n",
        "\n",
        "To enable GPU in Google Colab:\n",
        "1. Navigate to Edit→Notebook Settings or Runtime→Change Runtime Type.\n",
        "2. Select GPU from the Hardware Accelerator drop-down."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw688STi6Z6k"
      },
      "source": [
        "# Import the libraries needed for the exercise\n",
        "\n",
        "# Standard libraries\n",
        "import os\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Data manipulation and visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Deep Learning libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchsummary\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ffVUKx7P6RD"
      },
      "source": [
        "## Google Colab GPU\n",
        "Check that the GPU  enabled in your colab notebook by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPzhNed7P2i9",
        "outputId": "64453dad-cd61-4886-a15b-d06bd4a4b8ab"
      },
      "source": [
        "# Check is GPU is enabled\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: {}\".format(device))\n",
        "\n",
        "# Get specific GPU model\n",
        "if str(device) == \"cuda:0\":\n",
        "  print(\"GPU: {}\".format(torch.cuda.get_device_name(0)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL9_-2ilI0qv"
      },
      "source": [
        "## Mount Drive\n",
        "\n",
        "Mounting the drive will allow the Google Colab notebook to load and access files from your Google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P79X72ZCI2TA",
        "outputId": "73347cfc-7f24-4ddf-93d9-f8f4f559373f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIQAlBV9GbCZ"
      },
      "source": [
        "<a name=\"data-description\"></a>\n",
        "# Data Description\n",
        "\n",
        "In this section, you will learn how to:\n",
        "- Download the EuroSAT dataset into your Google Drive\n",
        "- Generate the train and test sets by splitting the EuroSAT dataset\n",
        "- Visualize a sample of the images and their LULC labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK0AM2bWGxsw"
      },
      "source": [
        "## EuroSAT Dataset\n",
        "The [EuroSAT dataset](https://github.com/phelber/EuroSAT) contains 27,000 labelled 64x64 pixel Sentinel-2 satellite image patches with 10 different LULC categories. Both RGB and multi-spectral (MS) images are available for download. For simplicity, we will focus on RGB image classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GNyqemwGwr9",
        "outputId": "5e104c97-899f-4fa0-dbdf-2c5bcb3ffb51"
      },
      "source": [
        "# Download Sentinal-2 staellite images from EuroSAT dataset, unzip the file\n",
        "\n",
        "!wget http://madm.dfki.de/files/sentinel/EuroSAT.zip -O EuroSAT.zip\n",
        "!unzip -q EuroSAT.zip -d 'EuroSAT/'\n",
        "!rm EuroSAT.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-15 14:59:48--  http://madm.dfki.de/files/sentinel/EuroSAT.zip\n",
            "Resolving madm.dfki.de (madm.dfki.de)... 131.246.195.183\n",
            "Connecting to madm.dfki.de (madm.dfki.de)|131.246.195.183|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94280567 (90M) [application/zip]\n",
            "Saving to: ‘EuroSAT.zip’\n",
            "\n",
            "EuroSAT.zip         100%[===================>]  89.91M  18.1MB/s    in 6.1s    \n",
            "\n",
            "2024-07-15 14:59:55 (14.8 MB/s) - ‘EuroSAT.zip’ saved [94280567/94280567]\n",
            "\n",
            "replace EuroSAT/2750/River/River_479.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp_i70hQ-ZcX"
      },
      "source": [
        "## Generate Train and Test Sets\n",
        "\n",
        "### Create Custom Dataset Class\n",
        "In Pytorch, the `Dataset` class allows you to define a custom class to load the input and target for a dataset.  We will use this capability to load in our inputs in the form of RGB satellite images along with their corresponding labels. Later we'll learn how to apply necessary image transformations (see next section)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsK-rDJyUFpV"
      },
      "source": [
        "class EuroSAT(data.Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Apply image transformations\n",
        "        if self.transform:\n",
        "            x = self.transform(dataset[index][0])\n",
        "        else:\n",
        "            x = dataset[index][0]\n",
        "        # Get class label\n",
        "        y = dataset[index][1]\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9bbA-_d-Vis"
      },
      "source": [
        "### Data Augmentation\n",
        "\n",
        "Data augmentation is a  technique that randomly applies image transformations, e.g. crops, horizontal flips, and vertical flips, to the input images during model training. These perturbations reduce the neural network's overfitting to the training dataset, and they allow it to generalize better to the unseen test dataset.\n",
        "<br><br>\n",
        "<center> <br>\n",
        "<center> <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/0*ttoU2HOnBI8cb9Y2.png\" width=\"400\"/>\n",
        "<br>\n",
        "<font size=2>Image Source: <a href=\"https://pranjal-ostwal.medium.com/data-augmentation-for-computer-vision-b88b818b6010\">https://pranjal-ostwal.medium.com/data-augmentation-for-computer-vision-b88b818b6010</a></a> </font>\n",
        "<br>\n",
        "</center>\n",
        "</center>\n",
        "<br>\n",
        "<font size=2>Image Source: Ahmad, Jamil & Muhammad, Khan & Baik, Sung. (2017). Data augmentation-assisted deep learning of hand-drawn partially colored sketches for visual search. PLOS ONE. 12. e0183838. 10.1371/journal.pone.0183838. </font>\n",
        "<br>\n",
        "\n",
        "\n",
        "### Image Normalization\n",
        "Additionally, in the cell below, the `transforms.Normalize` method normalizes each of the three channels to the given means and standard deviations defined in the `imagenet_mean` and `imagenet_std` variables. ImageNet is a large training dataset of images and labels.  Later in this tutorial, we will be using a model pre-trained on this dataset.  In order to use this pre-trained model for our LULC dataset, we need to ensure that the input dataset is normalized to have the same statistics (mean and standard deviation) as ImageNet.\n",
        "\n",
        "<br>\n",
        "<center> <img src=\"https://cv.gluon.ai/_images/imagenet_banner.jpeg\" width=\"400\"/>\n",
        "<br>\n",
        "<font size=2>Image Source: <a href=\" https://cv.gluon.ai/build/examples_datasets/imagenet.html\">https://cv.gluon.ai/build/examples_datasets/imagenet.html</a></font>\n",
        "<br>\n",
        "</center>\n",
        "\n",
        "Existing research has found that using models pretrained on massive datasets, such as ImageNet, improves accuracy when applying these neural networks to new datasets.  Pre-trained models serve as excellent generic feature extractors.  [Please read here for more information](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osNVdWjtIQMK"
      },
      "source": [
        "# Setting up the data preprocessing transformations needed for training, validation,\n",
        "# and testing datasets using the pytorch library\n",
        "\n",
        "input_size = 224\n",
        "imagenet_mean, imagenet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(input_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(input_size),\n",
        "    transforms.CenterCrop(input_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(input_size),\n",
        "    transforms.CenterCrop(input_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-H1Y1rHVz8t"
      },
      "source": [
        "### Load EuroSAT Dataset\n",
        "Let's start by loading the EuroSAT dataset using torch's `ImageFolder` class.\n",
        "\n",
        "`ImageFolder` is a generic data loader where the images are arranged in this way:\n",
        "\n",
        "```\n",
        "    data\n",
        "    └───AnnualCrop\n",
        "    │   │   AnnualCrop_1.jpg\n",
        "    │   │   AnnualCrop_2.jpg\n",
        "    │   │   AnnualCrop_3.jpg\n",
        "    │   │   ...\n",
        "    └───Forest\n",
        "    │   │   Forest_1.jpg\n",
        "    │   │   Forest_2.jpg\n",
        "    │   │   Forest_3.jpg\n",
        "    │   │   ...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM7hyGz3Ry8e"
      },
      "source": [
        "# Load the dataset\n",
        "data_dir = './EuroSAT/2750/'\n",
        "dataset = datasets.ImageFolder(data_dir)\n",
        "\n",
        "# Get LULC categories\n",
        "class_names = dataset.classes\n",
        "print(\"Class names: {}\".format(class_names))\n",
        "print(\"Total number of classes: {}\".format(len(class_names)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY8oTO459LXo"
      },
      "source": [
        "### Split into Train, Validation, and Test Sets\n",
        "Here, we split the dataset into a train set and test set. The training set will be 70% of the Eurosat dataset, randomly selected. We set aside 15% of the dataset as our validation set and the remaining 15% as our test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6_Jm_Vt89-L"
      },
      "source": [
        "# Apply different transformations to the training and test sets - like the cat image above\n",
        "train_data = EuroSAT(dataset, train_transform)\n",
        "val_data = EuroSAT(dataset, val_transform)\n",
        "test_data = EuroSAT(dataset, test_transform)\n",
        "\n",
        "# Randomly split the dataset into 70% train / 15% val / 15% test\n",
        "# by subsetting the transformed train and test datasets\n",
        "train_size = 0.70\n",
        "val_size = 0.15\n",
        "indices = list(range(int(len(dataset))))\n",
        "train_split = int(train_size * len(dataset))\n",
        "val_split = int(val_size * len(dataset))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_data = data.Subset(train_data, indices=indices[:train_split])\n",
        "val_data = data.Subset(val_data, indices=indices[train_split: train_split+val_split])\n",
        "test_data = data.Subset(test_data, indices=indices[train_split+val_split:])\n",
        "print(\"Train/val/test sizes: {}/{}/{}\".format(len(train_data), len(val_data), len(test_data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTD1GGkR8Ss4"
      },
      "source": [
        "Finally, we use `torch`'s `DataLoader` class to create a dataloader.  The dataloader manages fetching samples from the datasets (it can even fetch them in parallel using `num_workers`) and assembles batches of the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhoykOhv8LdN"
      },
      "source": [
        "num_workers = 2\n",
        "batch_size = 16\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "    train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True\n",
        ")\n",
        "val_loader = data.DataLoader(\n",
        "    val_data, batch_size=batch_size, num_workers=num_workers, shuffle=False\n",
        ")\n",
        "test_loader = data.DataLoader(\n",
        "    test_data, batch_size=batch_size, num_workers=num_workers, shuffle=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcjHBMnAawgS"
      },
      "source": [
        "## Visualize Data\n",
        "\n",
        "In the cell below, we will visualize a batch of the dataset.  The cell visualizes the input to the neural network (the RGB image) along with the associated label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qphYetp0ayIZ"
      },
      "source": [
        "# Using MatPlotLib to show a grid of some of the land types in the dataset\n",
        "\n",
        "n = 4\n",
        "inputs, classes = next(iter(train_loader))\n",
        "fig, axes = plt.subplots(n, n, figsize=(8, 8))\n",
        "\n",
        "for i in range(n):\n",
        "  for j in range(n):\n",
        "    image = inputs[i * n + j].numpy().transpose((1, 2, 0))\n",
        "    image = np.clip(np.array(imagenet_std) * image + np.array(imagenet_mean), 0, 1)\n",
        "\n",
        "    title = class_names[classes[i * n + j]]\n",
        "    axes[i, j].imshow(image)\n",
        "    axes[i, j].set_title(title)\n",
        "    axes[i, j].axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O-ZKD5KqdMm"
      },
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "Next, let's explore our dataset a little bit more.  In particular, how many images of each class are included?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhHebM9-qiAv"
      },
      "source": [
        "# Using MatPlotLib to create a histogram of land type\n",
        "\n",
        "plt.figure(figsize=(6, 3))\n",
        "hist = sns.histplot(dataset.targets)\n",
        "\n",
        "hist.set_xticks(range(len(dataset.classes)))\n",
        "hist.set_xticklabels(dataset.classes, rotation=90)\n",
        "hist.set_title('Histogram of Dataset Classes in EuroSAT Dataset')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ3ef1a34gix"
      },
      "source": [
        "# Model Development\n",
        "\n",
        "## Instantiate Model\n",
        "\n",
        "First, let's instatiate the model.  To start, we will use a standard neural network architecture, called ResNet50. Based on [the work by Helber et al.](https://arxiv.org/pdf/1709.00029.pdf), ResNet-50 has been shown to work well for LULC classification on the EuroSAT\n",
        "\n",
        "### ResNet-50\n",
        "<b>Recall</b>: Deep neural networks are difficult to train due to the problem of vanishing or exploding gradients (repeated multiplication making the gradient infinitively small). ResNet solves this by using shortcut connections that connect activation from an earlier layer to a further layer by skipping one or more layers as shown below. This allows for gradients to propagate to the deeper layers before they can be reduced to small or zero values.\n",
        "<br><br>\n",
        "\n",
        "<center> <img src=\"https://jananisbabu.github.io/ResNet50_From_Scratch_Tensorflow/images/resnet50.png\" width=\"600\"/><br>\n",
        "Image source: <a href=\"https://jananisbabu.github.io/ResNet50_From_Scratch_Tensorflow/\">https://jananisbabu.github.io/ResNet50_From_Scratch_Tensorflow/  </a>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "Note that when we load the model, we set the `weights=models.ResNet50_Weights.DEFAULT` to indicate that the loaded model should be already pre-trained on the Imagenet dataset. We also modify the final layer so that the output matches the number of classes in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPXbNl_TLHQQ"
      },
      "source": [
        "# set up a ResNet-50 model for image classification,\n",
        "# adapt it for the number of classes in the given dataset,\n",
        "# move it to a specific device (i.e., a GPU in this case),\n",
        "# and then display a summary of the model architecture.\n",
        "\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(dataset.classes))\n",
        "model = model.to(device)\n",
        "torchsummary.summary(model, (3, 224, 224))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGf__UVfNNOO"
      },
      "source": [
        "## Model Training and Evaluation\n",
        "\n",
        "We can now proceed to model training and evaluation.\n",
        "\n",
        "This section has three major parts:\n",
        "\n",
        "1. Specify the criterion, optimizer, and hyperparameters (e.g. n_epochs, learning rate, etc.).\n",
        "2. Train the model on the training set by updating its weights to minimize the loss function.\n",
        "3. Evaluate the model on the test set to observe performance on new, unseen data.\n",
        "4. Repeat steps 2 and 3 `n_epochs` times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3waPRdOai7AB"
      },
      "source": [
        "### Cross Entropy Loss\n",
        "We define our loss as the cross-entropy loss, which measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. ([Source](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html))\n",
        "\n",
        "For two classes, it is computed as:\n",
        "\n",
        "$−ylog(p)-(1−y)log(1−p)$\n",
        "\n",
        "For multiclass classification with $M$ classes, it is defined as:\n",
        "\n",
        "$−\\sum_{c=1}^{M}y_{o,c}log(p_{o,c})$\n",
        "\n",
        "where\n",
        "\n",
        "- $M$ - number of classes (dog, cat, fish)\n",
        "- $log$ - the natural log\n",
        "- $y_{o,c}$ - binary indicator (0 or 1) if class label $c$ is the classification for observation $o$\n",
        "- $p_{o,c}$- predicted probability observation $o$ is of class $c$\n",
        "\n",
        "### Stochastic Gradient Descent\n",
        "Remember that the goal of stochastic gradient descent (SGD) is to minimize the loss function. To do this, it computes the slope (gradient) of the loss function at the current point and moves in the opposite direction of the slope towards the steepest descent.\n",
        "<center> <img src=\"https://miro.medium.com/max/1400/1*P7z2BKhd0R-9uyn9ThDasA.png\" width=\"350\"/><br>Image source:\n",
        "<a href=\"https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a\">https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a</a>\n",
        "</center>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fap-QROui8Pr"
      },
      "source": [
        "# Specify number of epochs and learning rate\n",
        "n_epochs = 10\n",
        "lr = 1e-3\n",
        "\n",
        "# Specify criterion and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next. let's create our training function."
      ],
      "metadata": {
        "id": "par-N39lvb6Y"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE4entSvNc2q"
      },
      "source": [
        "# Create a training function\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer):\n",
        "  model.train()\n",
        "\n",
        "  running_loss = 0.0\n",
        "  running_total_correct = 0.0\n",
        "\n",
        "  for i, (inputs, labels) in enumerate(tqdm(dataloader)):\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Zero the parameter gradients\n",
        "    # Clear off previous weights in order\n",
        "    # to obtain updated weights.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Compute the gradients wrt the loss\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the weights based on the\n",
        "    # internally stored gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate statistics\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    # Calculate running loss and accuracy\n",
        "    running_loss += loss.item() * inputs.size(0)\n",
        "    running_total_correct += torch.sum(preds == labels)\n",
        "\n",
        "  # Calculate epoch loss and accuracy\n",
        "  epoch_loss = running_loss / len(dataloader.dataset)\n",
        "  epoch_accuracy = (running_total_correct / len(dataloader.dataset)) * 100\n",
        "  print(f\"Train Loss: {epoch_loss:.2f}; Accuracy: {epoch_accuracy:.2f}\")\n",
        "\n",
        "  return epoch_loss, epoch_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D2XrpGxtJ9w"
      },
      "source": [
        "Next, let's define the model evaluation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvjKlBsNP_pF"
      },
      "source": [
        "def evaluate(model, dataloader, criterion, phase=\"val\"):\n",
        "  model.eval()\n",
        "\n",
        "  running_loss = 0.0\n",
        "  running_total_correct = 0.0\n",
        "\n",
        "  for i, (inputs, labels) in enumerate(tqdm(dataloader)):\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    running_loss += loss.item() * inputs.size(0)\n",
        "    running_total_correct += torch.sum(preds == labels)\n",
        "\n",
        "  # Calculate epoch loss and accuracy\n",
        "  epoch_loss = running_loss / len(dataloader.dataset)\n",
        "  epoch_accuracy = (running_total_correct / len(dataloader.dataset)) * 100\n",
        "  print(f\"{phase.title()} Loss: {epoch_loss:.2f}; Accuracy: {epoch_accuracy:.2f}\")\n",
        "\n",
        "  return epoch_loss, epoch_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting it all together, we define the `fit` function for training and evaluating the model on the training set and validation set, respectively."
      ],
      "metadata": {
        "id": "z9Zj-K8uox_h"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNgDSfQDMSFI"
      },
      "source": [
        "def fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer):\n",
        "  # Keep track of the best loss and\n",
        "  # best model weights with the lowest loss\n",
        "  best_loss = np.inf\n",
        "  best_model = None\n",
        "\n",
        "  # Train and test over n_epochs\n",
        "  for epoch in range(n_epochs):\n",
        "    print(\"Epoch {}\".format(epoch+1))\n",
        "    train(model, train_loader, criterion, optimizer)\n",
        "    val_loss, _ = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "      best_loss = val_loss\n",
        "      best_model = model\n",
        "\n",
        "  return best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzTl0FX3tN8m"
      },
      "source": [
        "We can now commence model training and evaluation in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training - based on 10 epochs only (use more to make it more accurate in 'real world')\n",
        "\n",
        "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
      ],
      "metadata": {
        "id": "Uy0FvQkumiN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example, we only trained the model for 10 epochs. In practice, you'll want to train the model for much longer to achieve the best results."
      ],
      "metadata": {
        "id": "J3HPljIKqYyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Performance on the Test Set\n",
        "Using the best model from the previous steps, we can evaluate the model performance on the test set."
      ],
      "metadata": {
        "id": "o-4SbmYgu-ZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, _ = evaluate(best_model, test_loader, criterion, phase=\"test\")"
      ],
      "metadata": {
        "id": "Cdf5Qn8Juq4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14oPIiaqb-hI"
      },
      "source": [
        "## Save Model\n",
        "\n",
        "Let's define a function for saving the model to our local Google drive as follows.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = \"./drive/My Drive/Colab Notebooks/models/\"\n",
        "if not os.path.exists(model_dir):\n",
        "  os.makedirs(model_dir)\n",
        "\n",
        "model_file = os.path.join(model_dir, 'best_model.pth')\n",
        "model_file"
      ],
      "metadata": {
        "id": "riBvu9R8rV7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txqSMimEO5B1"
      },
      "source": [
        "def save_model(best_model, model_file):\n",
        "  torch.save(best_model.state_dict(), model_file)\n",
        "  print('Model successfully saved to {}.'.format(model_file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(best_model, model_file)"
      ],
      "metadata": {
        "id": "em2xesOoRNr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeqCoai8hcYr"
      },
      "source": [
        "## Load Model\n",
        "Here we show you how to load the saved model from the previous step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRRJ0vlCheDD",
        "collapsed": true
      },
      "source": [
        "def load_model(model_file):\n",
        "  # Uncomment this to download the model file\n",
        "  #if not os.path.isfile(model_file):\n",
        "  #  model_file = 'best_model.pth'\n",
        "  #  !gdown \"13AFOESwxKmexCoOeAbPSX_wr-hGOb9YY\"\n",
        "\n",
        "  model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "  model.fc = torch.nn.Linear(model.fc.in_features, 10)\n",
        "  model.load_state_dict(torch.load(model_file))\n",
        "  model.eval()\n",
        "\n",
        "  print('Model file {} successfully loaded.'.format(model_file))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_file)"
      ],
      "metadata": {
        "id": "9CnI7XOQSiP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6c20o6xcnZ6"
      },
      "source": [
        "<a name=\"results\"></a>\n",
        "# Results\n",
        "\n",
        "Let's visualize an example of the neural network making a prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77fAZ7xEglBX"
      },
      "source": [
        "# Retrieve sample image\n",
        "index = 15\n",
        "image, label = test_data[index]\n",
        "\n",
        "# Predict on sample\n",
        "model = model.to(\"cpu\")\n",
        "output = model(image.unsqueeze(0))\n",
        "_, pred = torch.max(output, 1)\n",
        "\n",
        "# Get corresponding class label\n",
        "label = class_names[label]\n",
        "pred = class_names[pred[0]]\n",
        "\n",
        "# Visualize sample and prediction\n",
        "image = image.cpu().numpy().transpose((1, 2, 0))\n",
        "image = np.clip(np.array(imagenet_std) * image + np.array(imagenet_mean), 0, 1)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(3, 3))\n",
        "ax.imshow(image)\n",
        "ax.set_title(\"Predicted class: {}\\nActual Class: {}\".format(pred, label));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvUL5a5Gp5fC"
      },
      "source": [
        "Here, we show how to run the model on a PIL image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuMyKEFjdvvn"
      },
      "source": [
        "from PIL import Image\n",
        "image_path = './EuroSAT/2750/Forest/Forest_2.jpg'\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Transform image\n",
        "input = test_transform(image)\n",
        "\n",
        "# Predict on sample\n",
        "output = model(input.unsqueeze(0))\n",
        "\n",
        "# Get corresponding class label\n",
        "_, pred = torch.max(output, 1)\n",
        "pred = class_names[pred[0]]\n",
        "\n",
        "# Visualize results\n",
        "fig, ax = plt.subplots(figsize=(3,3))\n",
        "ax.imshow(image)\n",
        "ax.set_title(\"Predicted class: {}\".format(pred));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"exercises\"></a>\n",
        "# Exercise 1: Experiment with a different fine-tuning strategy.\n",
        "\n",
        "So far, we've intialized the CNN with weights from a model train on the ImageNet data and retrained the model on the EuroSAT dataset by updating **all weights**. Another approach to fine-tuning involves using the pretrained convolutional layers as a fixed feature extractor and freezing those weights, only updating the weights of the final fully-connected layers for classification."
      ],
      "metadata": {
        "id": "RGWzTFVsVUy5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⭐ **YOUR TURN!:** Freeze all but the final layers of a ResNet50 model. How does this finetuning strategy compare against the previous strategy?"
      ],
      "metadata": {
        "id": "CZKCNy0LVlLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(dataset.classes))\n",
        "model = model.to(device)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "# Add final (unfrozen) layer for classification\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(dataset.classes))\n",
        "model = model.to(device)\n",
        "\n",
        "# Commence training\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
      ],
      "metadata": {
        "id": "NHrXwXxH5zfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution"
      ],
      "metadata": {
        "id": "cZzXC7IHCazM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(dataset.classes))\n",
        "model = model.to(device)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Add final (unfrozen) layer for classification\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(dataset.classes))\n",
        "model = model.to(device)\n",
        "\n",
        "# Commence training\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
      ],
      "metadata": {
        "id": "KrIB6R9BQ_g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2: Experiment with different Imagenet-pretrained CNN models."
      ],
      "metadata": {
        "id": "O83JctiPVJ4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet50 is only one of many different CNN model architectures. Determining the best model architecture is an important part of the model selection process, and in practice, it's best to try out and compare different model architectures. A detailed description of pretrained CNN models supported in Pytorch can be found here: https://pytorch.org/vision/0.18/models.html\n",
        "\n",
        "\n",
        "⭐ **YOUR TURN!:** Try a different pre-trained CNN model from Pytorch, based on the available models found [here](https://pytorch.org/vision/0.18/models.html). How well does it perform compared to ResNet50?\n",
        "\n",
        "Make sure to modify the last layer for classification to match the number of classes (Hint: the number of classes = `len(dataset.classes)`)."
      ],
      "metadata": {
        "id": "RTtAvwK9VO3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Modify the final layer for classification\n",
        "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, len(dataset.classes))\n",
        "\n",
        "model = model.to(device)\n",
        "torchsummary.summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "lie0tTMF-KdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Commence training\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
      ],
      "metadata": {
        "id": "WiuNJFl2-MOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution"
      ],
      "metadata": {
        "id": "ET8xcMK0WEeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we choose EfficientNet-B0 as the model to train and evaluate. More information on EfficientNets can be found [here](https://arxiv.org/pdf/1905.11946)."
      ],
      "metadata": {
        "id": "pNiMyUN3PZ9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Modify the final layer for classification\n",
        "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, len(dataset.classes))\n",
        "\n",
        "model = model.to(device)\n",
        "torchsummary.summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "I7Ybi3eEWF_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Commence training\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
      ],
      "metadata": {
        "id": "JwyC5pyLXRUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3: Experiment with Satellite Image-pretrained CNN models.\n",
        "\n",
        "Practitioners have traditionally relied on Imagenet-pretrained models for transfer learning tasks, even for Earth Observation data. However, with the increasing availability of remote sensing data, we're now seeing more and more large-scale remote sensing datasets being curated for model pre-training, such as [SSL4EO-S12](https://github.com/zhu-xlab/SSL4EO-S12) by Wang et al. Models trained on these datasets utilize unsupervised/self-supervised learning techniques, leveraging the large amounts of raw, unlabeled remote sensing data for model pretraining.\n",
        "\n",
        "These pretrained weights are made accessible via the [Torchgeo library](https://github.com/microsoft/torchgeo). More information on available pretrained weights can be found here: https://torchgeo.readthedocs.io/en/stable/api/models.html#pretrained-weights"
      ],
      "metadata": {
        "id": "L4pp77orVXOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⭐ **YOUR TURN!:** Using torchgeo, load a pretrained Sentinel-2 pretrained ResNet50 model and finetune the model by updating **all weights**. How does this model compare against the Imagenet-pretrained ResNet50 model?"
      ],
      "metadata": {
        "id": "1D07TUK9VmRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q timm\n",
        "!pip install -q torchgeo"
      ],
      "metadata": {
        "id": "5SLg99HeWC7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "from torchgeo.models import ResNet50_Weights\n",
        "\n",
        "weights = ResNet50_Weights.SENTINEL2_RGB_MOCO\n",
        "model = timm.create_model(\n",
        "    \"resnet50\", in_chans=weights.meta[\"in_chans\"],\n",
        "    num_classes=len(dataset.classes)\n",
        ")\n",
        "model.load_state_dict(weights.get_state_dict(progress=True), strict=False)\n",
        "\n",
        "model = model.to(device)\n",
        "torchsummary.summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "KdMt3TCFZI_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Commence training\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
      ],
      "metadata": {
        "id": "eTUvrzt8cJm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution"
      ],
      "metadata": {
        "id": "KLhzjCifTcW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "from torchgeo.models import ResNet50_Weights\n",
        "\n",
        "weights = ResNet50_Weights.SENTINEL2_RGB_MOCO\n",
        "model = timm.create_model(\n",
        "    \"resnet50\", in_chans=weights.meta[\"in_chans\"],\n",
        "    num_classes=len(dataset.classes)\n",
        ")\n",
        "model.load_state_dict(weights.get_state_dict(progress=True), strict=False)\n",
        "\n",
        "model = model.to(device)\n",
        "torchsummary.summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "HFRKhevPVZoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Commence training\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
      ],
      "metadata": {
        "id": "6rIIhjK2Yy3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congrats on making it this far! Go to the next section, [Part 2: Automating Land Use and Land Cover Mapping using Python.](https://colab.research.google.com/drive/13I1wZT7thBlNdGA1tFQrK1MlRhMZMnpM?usp=sharing)."
      ],
      "metadata": {
        "id": "fObmOlAL0o6v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYk0TXtZ0G4Y"
      },
      "source": [
        "# References\n",
        "- Helber, P., Bischke, B., Dengel, A., & Borth, D. (2019). Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7), 2217-2226.\n",
        "- Wang, Y., Braham, N. A. A., Xiong, Z., Liu, C., Albrecht, C. M., & Zhu, X. X. (2023). SSL4EO-S12: A large-scale multimodal, multitemporal dataset for self-supervised learning in Earth observation [Software and Data Sets]. IEEE Geoscience and Remote Sensing Magazine, 11(3), 98-106."
      ]
    }
  ]
}